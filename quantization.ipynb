{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow \n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Model on Mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#Load dataset as train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "num_classes = 10\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              785000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 1,796,010\n",
      "Trainable params: 1,796,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import models, layers\n",
    "from keras import regularizers\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dropout(0.2,input_shape=(784,)))\n",
    "model.add(keras.layers.Dense(1000,\n",
    "                        kernel_regularizer = regularizers.l2(0.01),\n",
    "                        activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(1000,\n",
    "                        kernel_regularizer = regularizers.l2(0.01),\n",
    "                        activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(10,  activation='softmax'))\n",
    "#display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A) Post Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 2.0541 - accuracy: 0.8565 - val_loss: 0.6929 - val_accuracy: 0.9219\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.7655 - accuracy: 0.8908 - val_loss: 0.6280 - val_accuracy: 0.9320\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.7309 - accuracy: 0.8975 - val_loss: 0.6130 - val_accuracy: 0.9349\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.7112 - accuracy: 0.9006 - val_loss: 0.6017 - val_accuracy: 0.9331\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.6863 - accuracy: 0.9056 - val_loss: 0.5729 - val_accuracy: 0.9405\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.6802 - accuracy: 0.9051 - val_loss: 0.5550 - val_accuracy: 0.9481\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.6707 - accuracy: 0.9057 - val_loss: 0.5376 - val_accuracy: 0.9507\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.6490 - accuracy: 0.9075 - val_loss: 0.5352 - val_accuracy: 0.9416\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.6426 - accuracy: 0.9086 - val_loss: 0.5558 - val_accuracy: 0.9427\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 3s 55us/sample - loss: 0.6419 - accuracy: 0.9082 - val_loss: 0.5148 - val_accuracy: 0.9492\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train,\n",
    "                        batch_size=128,\n",
    "                        epochs=10,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.5148 - accuracy: 0.9492\n",
      "Test loss 0.5148, accuracy 94.92%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test loss {:.4f}, accuracy {:.2f}%\".format(score[0], score[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#Save the entire model in model.h5 file\n",
    "model.save(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7185624"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('model.h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "#saving converted model in \"converted_model.tflite\" file\n",
    "open(\"converted_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1803744"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('model.h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "#saving converted model in \"converted_quant_model.tflite\" file\n",
    "open(\"converted_quant_model.tflite\", \"wb\").write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model in Mb: 6.852745056152344\n",
      "Quantized model in Mb: 1.720184326171875\n",
      "Compression ratio: 3.983727180797275\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Float model in Mb:\", os.path.getsize('converted_model.tflite') / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize('converted_quant_model.tflite') / float(2**20))\n",
    "print(\"Compression ratio:\", os.path.getsize('converted_model.tflite')/os.path.getsize('converted_quant_model.tflite'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.6\n"
     ]
    }
   ],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = \\\n",
    "tf.lite.Interpreter(model_path=\"converted_quant_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "# Test model on some input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "acc=0\n",
    "for i in range(len(x_test)):\n",
    "    input_data = x_test[i].reshape(input_shape)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    if(np.argmax(output_data) == np.argmax(y_test[i])):\n",
    "        acc+=1\n",
    "acc = acc/len(x_test)\n",
    "print(acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B) Quantization Aware Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_12 (Dropout)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1000)              785000    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 1,796,010\n",
      "Trainable params: 1,796,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import models, layers\n",
    "from keras import regularizers\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dropout(0.2,input_shape=(784,)))\n",
    "model.add(keras.layers.Dense(1000,\n",
    "                        kernel_regularizer = regularizers.l2(0.01),\n",
    "                        activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(1000,\n",
    "                        kernel_regularizer = regularizers.l2(0.01),\n",
    "                        activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(10,  activation='softmax'))\n",
    "#display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/100\n",
      "54000/54000 [==============================] - 4s 66us/sample - loss: 2.1969 - accuracy: 0.8506 - val_loss: 0.6505 - val_accuracy: 0.9327\n",
      "Epoch 2/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.7705 - accuracy: 0.8896 - val_loss: 0.6030 - val_accuracy: 0.9443\n",
      "Epoch 3/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.7433 - accuracy: 0.8951 - val_loss: 0.6016 - val_accuracy: 0.9423\n",
      "Epoch 4/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.7206 - accuracy: 0.9001 - val_loss: 0.5755 - val_accuracy: 0.9528\n",
      "Epoch 5/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.7063 - accuracy: 0.9022 - val_loss: 0.5372 - val_accuracy: 0.9603\n",
      "Epoch 6/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.6881 - accuracy: 0.9041 - val_loss: 0.5492 - val_accuracy: 0.9567\n",
      "Epoch 7/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.6780 - accuracy: 0.9051 - val_loss: 0.5072 - val_accuracy: 0.9607\n",
      "Epoch 8/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.6712 - accuracy: 0.9061 - val_loss: 0.5015 - val_accuracy: 0.9612\n",
      "Epoch 9/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.6554 - accuracy: 0.9079 - val_loss: 0.5030 - val_accuracy: 0.9600\n",
      "Epoch 10/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.6475 - accuracy: 0.9085 - val_loss: 0.5101 - val_accuracy: 0.9555\n",
      "Epoch 11/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.6386 - accuracy: 0.9092 - val_loss: 0.4867 - val_accuracy: 0.9585\n",
      "Epoch 12/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.6285 - accuracy: 0.9108 - val_loss: 0.4546 - val_accuracy: 0.9658\n",
      "Epoch 13/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.6239 - accuracy: 0.9069 - val_loss: 0.4703 - val_accuracy: 0.9610\n",
      "Epoch 14/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.6144 - accuracy: 0.9103 - val_loss: 0.4576 - val_accuracy: 0.9643\n",
      "Epoch 15/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.6111 - accuracy: 0.9103 - val_loss: 0.4701 - val_accuracy: 0.9565\n",
      "Epoch 16/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.6153 - accuracy: 0.9093 - val_loss: 0.4610 - val_accuracy: 0.9605\n",
      "Epoch 17/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.6100 - accuracy: 0.9100 - val_loss: 0.4647 - val_accuracy: 0.9585\n",
      "Epoch 18/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.6064 - accuracy: 0.9098 - val_loss: 0.4564 - val_accuracy: 0.9575\n",
      "Epoch 19/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.6030 - accuracy: 0.9096 - val_loss: 0.4519 - val_accuracy: 0.9603\n",
      "Epoch 20/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.6033 - accuracy: 0.9111 - val_loss: 0.4558 - val_accuracy: 0.9597\n",
      "Epoch 21/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5993 - accuracy: 0.9108 - val_loss: 0.4293 - val_accuracy: 0.9655\n",
      "Epoch 22/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5965 - accuracy: 0.9120 - val_loss: 0.4271 - val_accuracy: 0.9678\n",
      "Epoch 23/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5942 - accuracy: 0.9106 - val_loss: 0.4263 - val_accuracy: 0.9643\n",
      "Epoch 24/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5990 - accuracy: 0.9107 - val_loss: 0.4475 - val_accuracy: 0.9580\n",
      "Epoch 25/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5933 - accuracy: 0.9114 - val_loss: 0.4283 - val_accuracy: 0.9612\n",
      "Epoch 26/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5964 - accuracy: 0.9095 - val_loss: 0.4360 - val_accuracy: 0.9630\n",
      "Epoch 27/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5993 - accuracy: 0.9092 - val_loss: 0.4442 - val_accuracy: 0.9613\n",
      "Epoch 28/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5847 - accuracy: 0.9119 - val_loss: 0.4277 - val_accuracy: 0.9660\n",
      "Epoch 29/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5910 - accuracy: 0.9093 - val_loss: 0.4348 - val_accuracy: 0.9610\n",
      "Epoch 30/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5869 - accuracy: 0.9119 - val_loss: 0.4209 - val_accuracy: 0.9652\n",
      "Epoch 31/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5913 - accuracy: 0.9089 - val_loss: 0.4251 - val_accuracy: 0.9637\n",
      "Epoch 32/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5782 - accuracy: 0.9108 - val_loss: 0.4268 - val_accuracy: 0.9603\n",
      "Epoch 33/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5833 - accuracy: 0.9089 - val_loss: 0.4285 - val_accuracy: 0.9612\n",
      "Epoch 34/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5840 - accuracy: 0.9097 - val_loss: 0.4263 - val_accuracy: 0.9605\n",
      "Epoch 35/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5801 - accuracy: 0.9094 - val_loss: 0.4233 - val_accuracy: 0.9622\n",
      "Epoch 36/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5775 - accuracy: 0.9100 - val_loss: 0.4081 - val_accuracy: 0.9662\n",
      "Epoch 37/100\n",
      "54000/54000 [==============================] - 3s 56us/sample - loss: 0.5728 - accuracy: 0.9099 - val_loss: 0.4168 - val_accuracy: 0.9603\n",
      "Epoch 38/100\n",
      "54000/54000 [==============================] - 3s 59us/sample - loss: 0.5808 - accuracy: 0.9098 - val_loss: 0.3987 - val_accuracy: 0.9668\n",
      "Epoch 39/100\n",
      "54000/54000 [==============================] - 3s 57us/sample - loss: 0.5650 - accuracy: 0.9130 - val_loss: 0.4096 - val_accuracy: 0.9648\n",
      "Epoch 40/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5742 - accuracy: 0.9097 - val_loss: 0.4125 - val_accuracy: 0.9628\n",
      "Epoch 41/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5790 - accuracy: 0.9086 - val_loss: 0.3949 - val_accuracy: 0.9648\n",
      "Epoch 42/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5699 - accuracy: 0.9096 - val_loss: 0.4157 - val_accuracy: 0.9648\n",
      "Epoch 43/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5735 - accuracy: 0.9086 - val_loss: 0.4031 - val_accuracy: 0.9662\n",
      "Epoch 44/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5676 - accuracy: 0.9110 - val_loss: 0.4117 - val_accuracy: 0.9627\n",
      "Epoch 45/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5689 - accuracy: 0.9114 - val_loss: 0.3999 - val_accuracy: 0.9650\n",
      "Epoch 46/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5632 - accuracy: 0.9113 - val_loss: 0.3907 - val_accuracy: 0.9670\n",
      "Epoch 47/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5734 - accuracy: 0.9090 - val_loss: 0.4014 - val_accuracy: 0.9658\n",
      "Epoch 48/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5628 - accuracy: 0.9127 - val_loss: 0.4055 - val_accuracy: 0.9628\n",
      "Epoch 49/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5650 - accuracy: 0.9116 - val_loss: 0.3981 - val_accuracy: 0.9668\n",
      "Epoch 50/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5634 - accuracy: 0.9115 - val_loss: 0.3968 - val_accuracy: 0.9675\n",
      "Epoch 51/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5706 - accuracy: 0.9092 - val_loss: 0.3927 - val_accuracy: 0.9612\n",
      "Epoch 52/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5608 - accuracy: 0.9110 - val_loss: 0.3949 - val_accuracy: 0.9655\n",
      "Epoch 53/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5607 - accuracy: 0.9125 - val_loss: 0.3915 - val_accuracy: 0.9648\n",
      "Epoch 54/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5698 - accuracy: 0.9104 - val_loss: 0.3925 - val_accuracy: 0.9643\n",
      "Epoch 55/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5610 - accuracy: 0.9099 - val_loss: 0.3914 - val_accuracy: 0.9667\n",
      "Epoch 56/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5596 - accuracy: 0.9103 - val_loss: 0.3844 - val_accuracy: 0.9672\n",
      "Epoch 57/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5652 - accuracy: 0.9089 - val_loss: 0.3929 - val_accuracy: 0.9647\n",
      "Epoch 58/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5644 - accuracy: 0.9090 - val_loss: 0.3862 - val_accuracy: 0.9668\n",
      "Epoch 59/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5594 - accuracy: 0.9112 - val_loss: 0.3879 - val_accuracy: 0.9672\n",
      "Epoch 60/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5571 - accuracy: 0.9113 - val_loss: 0.3803 - val_accuracy: 0.9638\n",
      "Epoch 61/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5591 - accuracy: 0.9089 - val_loss: 0.3785 - val_accuracy: 0.9643\n",
      "Epoch 62/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5604 - accuracy: 0.9101 - val_loss: 0.3892 - val_accuracy: 0.9658\n",
      "Epoch 63/100\n",
      "54000/54000 [==============================] - 3s 59us/sample - loss: 0.5610 - accuracy: 0.9097 - val_loss: 0.3886 - val_accuracy: 0.9642\n",
      "Epoch 64/100\n",
      "54000/54000 [==============================] - 3s 61us/sample - loss: 0.5693 - accuracy: 0.9073 - val_loss: 0.3951 - val_accuracy: 0.9610\n",
      "Epoch 65/100\n",
      "54000/54000 [==============================] - 3s 60us/sample - loss: 0.5602 - accuracy: 0.9099 - val_loss: 0.3834 - val_accuracy: 0.9660\n",
      "Epoch 66/100\n",
      "54000/54000 [==============================] - 3s 56us/sample - loss: 0.5586 - accuracy: 0.9097 - val_loss: 0.3781 - val_accuracy: 0.9663\n",
      "Epoch 67/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5631 - accuracy: 0.9072 - val_loss: 0.3847 - val_accuracy: 0.9668\n",
      "Epoch 68/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5586 - accuracy: 0.9094 - val_loss: 0.3938 - val_accuracy: 0.9637\n",
      "Epoch 69/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5550 - accuracy: 0.9097 - val_loss: 0.3855 - val_accuracy: 0.9628\n",
      "Epoch 70/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5577 - accuracy: 0.9088 - val_loss: 0.3843 - val_accuracy: 0.9623\n",
      "Epoch 71/100\n",
      "54000/54000 [==============================] - 3s 56us/sample - loss: 0.5540 - accuracy: 0.9108 - val_loss: 0.3874 - val_accuracy: 0.9657\n",
      "Epoch 72/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5520 - accuracy: 0.9092 - val_loss: 0.3880 - val_accuracy: 0.9628\n",
      "Epoch 73/100\n",
      "54000/54000 [==============================] - 3s 56us/sample - loss: 0.5583 - accuracy: 0.9085 - val_loss: 0.3763 - val_accuracy: 0.9677\n",
      "Epoch 74/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5537 - accuracy: 0.9097 - val_loss: 0.3804 - val_accuracy: 0.9633\n",
      "Epoch 75/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5574 - accuracy: 0.9086 - val_loss: 0.3923 - val_accuracy: 0.9598\n",
      "Epoch 76/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5581 - accuracy: 0.9096 - val_loss: 0.3710 - val_accuracy: 0.9693\n",
      "Epoch 77/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5535 - accuracy: 0.9104 - val_loss: 0.3785 - val_accuracy: 0.9685\n",
      "Epoch 78/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5549 - accuracy: 0.9095 - val_loss: 0.3853 - val_accuracy: 0.9638\n",
      "Epoch 79/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5511 - accuracy: 0.9094 - val_loss: 0.3790 - val_accuracy: 0.9685\n",
      "Epoch 80/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5581 - accuracy: 0.9101 - val_loss: 0.3755 - val_accuracy: 0.9657\n",
      "Epoch 81/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5539 - accuracy: 0.9095 - val_loss: 0.3822 - val_accuracy: 0.9653\n",
      "Epoch 82/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5511 - accuracy: 0.9100 - val_loss: 0.3741 - val_accuracy: 0.9640\n",
      "Epoch 83/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5486 - accuracy: 0.9102 - val_loss: 0.3877 - val_accuracy: 0.9630\n",
      "Epoch 84/100\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5506 - accuracy: 0.9097 - val_loss: 0.3982 - val_accuracy: 0.9545\n",
      "Epoch 85/100\n",
      "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5549 - accuracy: 0.9094 - val_loss: 0.3702 - val_accuracy: 0.9655\n",
      "Epoch 86/100\n",
      "54000/54000 [==============================] - 3s 52us/sample - loss: 0.5486 - accuracy: 0.9102 - val_loss: 0.3743 - val_accuracy: 0.9668\n",
      "Epoch 87/100\n",
      "54000/54000 [==============================] - 3s 52us/sample - loss: 0.5486 - accuracy: 0.9101 - val_loss: 0.3881 - val_accuracy: 0.9623\n",
      "Epoch 88/100\n",
      "54000/54000 [==============================] - 3s 52us/sample - loss: 0.5501 - accuracy: 0.9078 - val_loss: 0.3869 - val_accuracy: 0.9643\n",
      "Epoch 89/100\n",
      "54000/54000 [==============================] - 3s 51us/sample - loss: 0.5522 - accuracy: 0.9107 - val_loss: 0.3717 - val_accuracy: 0.9657\n",
      "Epoch 90/100\n",
      "54000/54000 [==============================] - 3s 52us/sample - loss: 0.5521 - accuracy: 0.9081 - val_loss: 0.3783 - val_accuracy: 0.9667\n",
      "Epoch 91/100\n",
      "54000/54000 [==============================] - 3s 51us/sample - loss: 0.5467 - accuracy: 0.9096 - val_loss: 0.3775 - val_accuracy: 0.9652\n",
      "Epoch 92/100\n",
      "54000/54000 [==============================] - 3s 51us/sample - loss: 0.5491 - accuracy: 0.9104 - val_loss: 0.3652 - val_accuracy: 0.9705\n",
      "Epoch 93/100\n",
      "54000/54000 [==============================] - 3s 52us/sample - loss: 0.5477 - accuracy: 0.9103 - val_loss: 0.3717 - val_accuracy: 0.9665\n",
      "Epoch 94/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5560 - accuracy: 0.9077 - val_loss: 0.3792 - val_accuracy: 0.9643\n",
      "Epoch 95/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5444 - accuracy: 0.9114 - val_loss: 0.3740 - val_accuracy: 0.9698\n",
      "Epoch 96/100\n",
      "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5520 - accuracy: 0.9092 - val_loss: 0.3757 - val_accuracy: 0.9650\n",
      "Epoch 97/100\n",
      "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5519 - accuracy: 0.9079 - val_loss: 0.3843 - val_accuracy: 0.9615\n",
      "Epoch 98/100\n",
      "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5482 - accuracy: 0.9107 - val_loss: 0.3739 - val_accuracy: 0.9675\n",
      "Epoch 99/100\n",
      "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5520 - accuracy: 0.9086 - val_loss: 0.3720 - val_accuracy: 0.9668\n",
      "Epoch 100/100\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5445 - accuracy: 0.9104 - val_loss: 0.3723 - val_accuracy: 0.9693\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train,\n",
    "                        batch_size=128,\n",
    "                        epochs=100,\n",
    "                        verbose=1,\n",
    "                        validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f6b8a058ed0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f6b8a058ed0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f6b89faab50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f6b89faab50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f6b89fb0e50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f6b89fb0e50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f6b89fb7450>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f6b89fb7450>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f6b8a012550>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f6b8a012550>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f6b8a018750>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f6b8a018750>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quant_dropout_12 (QuantizeWr (None, 784)               1         \n",
      "_________________________________________________________________\n",
      "quant_dense_12 (QuantizeWrap (None, 1000)              785005    \n",
      "_________________________________________________________________\n",
      "quant_dropout_13 (QuantizeWr (None, 1000)              1         \n",
      "_________________________________________________________________\n",
      "quant_dense_13 (QuantizeWrap (None, 1000)              1001005   \n",
      "_________________________________________________________________\n",
      "quant_dropout_14 (QuantizeWr (None, 1000)              1         \n",
      "_________________________________________________________________\n",
      "quant_dense_14 (QuantizeWrap (None, 10)                10015     \n",
      "=================================================================\n",
      "Total params: 1,796,028\n",
      "Trainable params: 1,796,010\n",
      "Non-trainable params: 18\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# q_aware stands for for quantization aware.\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.6439 - accuracy: 0.8917 - val_loss: 0.3743 - val_accuracy: 0.9677\n",
      "Epoch 2/100\n",
      "54000/54000 [==============================] - 4s 82us/sample - loss: 0.5447 - accuracy: 0.9074 - val_loss: 0.3573 - val_accuracy: 0.9682\n",
      "Epoch 3/100\n",
      "54000/54000 [==============================] - 4s 83us/sample - loss: 0.5390 - accuracy: 0.9082 - val_loss: 0.3609 - val_accuracy: 0.9657\n",
      "Epoch 4/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5355 - accuracy: 0.9097 - val_loss: 0.3540 - val_accuracy: 0.9677\n",
      "Epoch 5/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5415 - accuracy: 0.9083 - val_loss: 0.3755 - val_accuracy: 0.9632\n",
      "Epoch 6/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5401 - accuracy: 0.9084 - val_loss: 0.3655 - val_accuracy: 0.9648\n",
      "Epoch 7/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5366 - accuracy: 0.9091 - val_loss: 0.3604 - val_accuracy: 0.9665\n",
      "Epoch 8/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5374 - accuracy: 0.9105 - val_loss: 0.3662 - val_accuracy: 0.9623\n",
      "Epoch 9/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5446 - accuracy: 0.9086 - val_loss: 0.3706 - val_accuracy: 0.9647\n",
      "Epoch 10/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5383 - accuracy: 0.9104 - val_loss: 0.3633 - val_accuracy: 0.9653\n",
      "Epoch 11/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5394 - accuracy: 0.9099 - val_loss: 0.3619 - val_accuracy: 0.9682\n",
      "Epoch 12/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5457 - accuracy: 0.9081 - val_loss: 0.3712 - val_accuracy: 0.9670\n",
      "Epoch 13/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5423 - accuracy: 0.9101 - val_loss: 0.3681 - val_accuracy: 0.9650\n",
      "Epoch 14/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5402 - accuracy: 0.9111 - val_loss: 0.3635 - val_accuracy: 0.9667\n",
      "Epoch 15/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5411 - accuracy: 0.9097 - val_loss: 0.3706 - val_accuracy: 0.9632\n",
      "Epoch 16/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5469 - accuracy: 0.9084 - val_loss: 0.3721 - val_accuracy: 0.9640\n",
      "Epoch 17/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5460 - accuracy: 0.9082 - val_loss: 0.3656 - val_accuracy: 0.9652\n",
      "Epoch 18/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5483 - accuracy: 0.9084 - val_loss: 0.3747 - val_accuracy: 0.9657\n",
      "Epoch 19/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5477 - accuracy: 0.9063 - val_loss: 0.3717 - val_accuracy: 0.9643\n",
      "Epoch 20/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5489 - accuracy: 0.9071 - val_loss: 0.3636 - val_accuracy: 0.9662\n",
      "Epoch 21/100\n",
      "54000/54000 [==============================] - 5s 84us/sample - loss: 0.5476 - accuracy: 0.9084 - val_loss: 0.3960 - val_accuracy: 0.9555\n",
      "Epoch 22/100\n",
      "54000/54000 [==============================] - 5s 85us/sample - loss: 0.5431 - accuracy: 0.9090 - val_loss: 0.3756 - val_accuracy: 0.9620\n",
      "Epoch 23/100\n",
      "54000/54000 [==============================] - 4s 82us/sample - loss: 0.5419 - accuracy: 0.9089 - val_loss: 0.3668 - val_accuracy: 0.9678\n",
      "Epoch 24/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5436 - accuracy: 0.9089 - val_loss: 0.3580 - val_accuracy: 0.9690\n",
      "Epoch 25/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5418 - accuracy: 0.9097 - val_loss: 0.3646 - val_accuracy: 0.9668\n",
      "Epoch 26/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5431 - accuracy: 0.9093 - val_loss: 0.3683 - val_accuracy: 0.9657\n",
      "Epoch 27/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5447 - accuracy: 0.9074 - val_loss: 0.3645 - val_accuracy: 0.9632\n",
      "Epoch 28/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5370 - accuracy: 0.9102 - val_loss: 0.3641 - val_accuracy: 0.9658\n",
      "Epoch 29/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5476 - accuracy: 0.9074 - val_loss: 0.3745 - val_accuracy: 0.9647\n",
      "Epoch 30/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5405 - accuracy: 0.9102 - val_loss: 0.3576 - val_accuracy: 0.9683\n",
      "Epoch 31/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5398 - accuracy: 0.9104 - val_loss: 0.3587 - val_accuracy: 0.9663\n",
      "Epoch 32/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5468 - accuracy: 0.9076 - val_loss: 0.3714 - val_accuracy: 0.9688\n",
      "Epoch 33/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5392 - accuracy: 0.9105 - val_loss: 0.3673 - val_accuracy: 0.9637\n",
      "Epoch 34/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5491 - accuracy: 0.9070 - val_loss: 0.3579 - val_accuracy: 0.9677\n",
      "Epoch 35/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5416 - accuracy: 0.9097 - val_loss: 0.3631 - val_accuracy: 0.9653\n",
      "Epoch 36/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5435 - accuracy: 0.9070 - val_loss: 0.3674 - val_accuracy: 0.9642\n",
      "Epoch 37/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5483 - accuracy: 0.9080 - val_loss: 0.3701 - val_accuracy: 0.9633\n",
      "Epoch 38/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5421 - accuracy: 0.9080 - val_loss: 0.3697 - val_accuracy: 0.9662\n",
      "Epoch 39/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5465 - accuracy: 0.9073 - val_loss: 0.3712 - val_accuracy: 0.9665\n",
      "Epoch 40/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5395 - accuracy: 0.9098 - val_loss: 0.3593 - val_accuracy: 0.9677\n",
      "Epoch 41/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5466 - accuracy: 0.9087 - val_loss: 0.3589 - val_accuracy: 0.9670\n",
      "Epoch 42/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5418 - accuracy: 0.9083 - val_loss: 0.3673 - val_accuracy: 0.9653\n",
      "Epoch 43/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5429 - accuracy: 0.9096 - val_loss: 0.3608 - val_accuracy: 0.9672\n",
      "Epoch 44/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5393 - accuracy: 0.9087 - val_loss: 0.3814 - val_accuracy: 0.9605\n",
      "Epoch 45/100\n",
      "54000/54000 [==============================] - 4s 80us/sample - loss: 0.5462 - accuracy: 0.9083 - val_loss: 0.3517 - val_accuracy: 0.9667\n",
      "Epoch 46/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5427 - accuracy: 0.9082 - val_loss: 0.3721 - val_accuracy: 0.9633\n",
      "Epoch 47/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5424 - accuracy: 0.9099 - val_loss: 0.3645 - val_accuracy: 0.9627\n",
      "Epoch 48/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5412 - accuracy: 0.9074 - val_loss: 0.3462 - val_accuracy: 0.9678\n",
      "Epoch 49/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5469 - accuracy: 0.9063 - val_loss: 0.3627 - val_accuracy: 0.9630\n",
      "Epoch 50/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5376 - accuracy: 0.9092 - val_loss: 0.3583 - val_accuracy: 0.9638\n",
      "Epoch 51/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5411 - accuracy: 0.9076 - val_loss: 0.3697 - val_accuracy: 0.9627\n",
      "Epoch 52/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5428 - accuracy: 0.9080 - val_loss: 0.3626 - val_accuracy: 0.9638\n",
      "Epoch 53/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5346 - accuracy: 0.9093 - val_loss: 0.3675 - val_accuracy: 0.9633\n",
      "Epoch 54/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5421 - accuracy: 0.9079 - val_loss: 0.3535 - val_accuracy: 0.9682\n",
      "Epoch 55/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5422 - accuracy: 0.9081 - val_loss: 0.3585 - val_accuracy: 0.9673\n",
      "Epoch 56/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5408 - accuracy: 0.9078 - val_loss: 0.3557 - val_accuracy: 0.9650\n",
      "Epoch 57/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5495 - accuracy: 0.9060 - val_loss: 0.3585 - val_accuracy: 0.9653\n",
      "Epoch 58/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5425 - accuracy: 0.9077 - val_loss: 0.3554 - val_accuracy: 0.9667\n",
      "Epoch 59/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5331 - accuracy: 0.9104 - val_loss: 0.3520 - val_accuracy: 0.9657\n",
      "Epoch 60/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5348 - accuracy: 0.9086 - val_loss: 0.3574 - val_accuracy: 0.9667\n",
      "Epoch 61/100\n",
      "54000/54000 [==============================] - 4s 80us/sample - loss: 0.5426 - accuracy: 0.9073 - val_loss: 0.3649 - val_accuracy: 0.9613\n",
      "Epoch 62/100\n",
      "54000/54000 [==============================] - 4s 80us/sample - loss: 0.5421 - accuracy: 0.9058 - val_loss: 0.3653 - val_accuracy: 0.9642\n",
      "Epoch 63/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5317 - accuracy: 0.9091 - val_loss: 0.3510 - val_accuracy: 0.9668\n",
      "Epoch 64/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5417 - accuracy: 0.9079 - val_loss: 0.3554 - val_accuracy: 0.9673\n",
      "Epoch 65/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5340 - accuracy: 0.9108 - val_loss: 0.3584 - val_accuracy: 0.9650\n",
      "Epoch 66/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5368 - accuracy: 0.9074 - val_loss: 0.3532 - val_accuracy: 0.9652\n",
      "Epoch 67/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5312 - accuracy: 0.9091 - val_loss: 0.3558 - val_accuracy: 0.9658\n",
      "Epoch 68/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5392 - accuracy: 0.9074 - val_loss: 0.3606 - val_accuracy: 0.9663\n",
      "Epoch 69/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5418 - accuracy: 0.9081 - val_loss: 0.3557 - val_accuracy: 0.9645\n",
      "Epoch 70/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5417 - accuracy: 0.9070 - val_loss: 0.3618 - val_accuracy: 0.9663\n",
      "Epoch 71/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5394 - accuracy: 0.9087 - val_loss: 0.3654 - val_accuracy: 0.9640\n",
      "Epoch 72/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5409 - accuracy: 0.9076 - val_loss: 0.3496 - val_accuracy: 0.9687\n",
      "Epoch 73/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5365 - accuracy: 0.9077 - val_loss: 0.3543 - val_accuracy: 0.9682\n",
      "Epoch 74/100\n",
      "54000/54000 [==============================] - 5s 85us/sample - loss: 0.5424 - accuracy: 0.9062 - val_loss: 0.3604 - val_accuracy: 0.9642\n",
      "Epoch 75/100\n",
      "54000/54000 [==============================] - 4s 80us/sample - loss: 0.5382 - accuracy: 0.9075 - val_loss: 0.3671 - val_accuracy: 0.9642\n",
      "Epoch 76/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5371 - accuracy: 0.9083 - val_loss: 0.3633 - val_accuracy: 0.9632\n",
      "Epoch 77/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5351 - accuracy: 0.9095 - val_loss: 0.3514 - val_accuracy: 0.9685\n",
      "Epoch 78/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5397 - accuracy: 0.9073 - val_loss: 0.3511 - val_accuracy: 0.9685\n",
      "Epoch 79/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5382 - accuracy: 0.9093 - val_loss: 0.3580 - val_accuracy: 0.9645\n",
      "Epoch 80/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5419 - accuracy: 0.9067 - val_loss: 0.3607 - val_accuracy: 0.9640\n",
      "Epoch 81/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5388 - accuracy: 0.9080 - val_loss: 0.3499 - val_accuracy: 0.9683\n",
      "Epoch 82/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5300 - accuracy: 0.9101 - val_loss: 0.3522 - val_accuracy: 0.9687\n",
      "Epoch 83/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5388 - accuracy: 0.9072 - val_loss: 0.3573 - val_accuracy: 0.9668\n",
      "Epoch 84/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5360 - accuracy: 0.9094 - val_loss: 0.3507 - val_accuracy: 0.9673\n",
      "Epoch 85/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5366 - accuracy: 0.9067 - val_loss: 0.3611 - val_accuracy: 0.9620\n",
      "Epoch 86/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5432 - accuracy: 0.9050 - val_loss: 0.3580 - val_accuracy: 0.9665\n",
      "Epoch 87/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5403 - accuracy: 0.9061 - val_loss: 0.3565 - val_accuracy: 0.9657\n",
      "Epoch 88/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5378 - accuracy: 0.9070 - val_loss: 0.3510 - val_accuracy: 0.9682\n",
      "Epoch 89/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5300 - accuracy: 0.9086 - val_loss: 0.3429 - val_accuracy: 0.9673\n",
      "Epoch 90/100\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.5347 - accuracy: 0.9080 - val_loss: 0.3555 - val_accuracy: 0.9642\n",
      "Epoch 91/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5396 - accuracy: 0.9070 - val_loss: 0.3635 - val_accuracy: 0.9640\n",
      "Epoch 92/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5430 - accuracy: 0.9073 - val_loss: 0.3460 - val_accuracy: 0.9667\n",
      "Epoch 93/100\n",
      "54000/54000 [==============================] - 4s 80us/sample - loss: 0.5373 - accuracy: 0.9081 - val_loss: 0.3617 - val_accuracy: 0.9650\n",
      "Epoch 94/100\n",
      "54000/54000 [==============================] - 5s 85us/sample - loss: 0.5414 - accuracy: 0.9067 - val_loss: 0.3540 - val_accuracy: 0.9670\n",
      "Epoch 95/100\n",
      "54000/54000 [==============================] - 5s 85us/sample - loss: 0.5438 - accuracy: 0.9058 - val_loss: 0.3563 - val_accuracy: 0.9672\n",
      "Epoch 96/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5392 - accuracy: 0.9085 - val_loss: 0.3585 - val_accuracy: 0.9652\n",
      "Epoch 97/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5399 - accuracy: 0.9068 - val_loss: 0.3552 - val_accuracy: 0.9665\n",
      "Epoch 98/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5378 - accuracy: 0.9059 - val_loss: 0.3539 - val_accuracy: 0.9652\n",
      "Epoch 99/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5435 - accuracy: 0.9070 - val_loss: 0.3553 - val_accuracy: 0.9657\n",
      "Epoch 100/100\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.5369 - accuracy: 0.9070 - val_loss: 0.3564 - val_accuracy: 0.9643\n"
     ]
    }
   ],
   "source": [
    "histq = q_aware_model.fit(x_train, y_train,\n",
    "                        batch_size=128,\n",
    "                        epochs=100,\n",
    "                        verbose=1,\n",
    "                        validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 101us/sample - loss: 0.4041 - accuracy: 0.9565\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.3832 - accuracy: 0.9559\n",
      "Baseline test accuracy: 95.6499993801117\n",
      "Quant test accuracy: 95.59000134468079\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    x_test, y_test, verbose=1)\n",
    "\n",
    "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
    "    x_test, y_test, verbose=1)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy*100)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### fine tune with QAT on a subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 900 samples, validate on 100 samples\n",
      "Epoch 1/100\n",
      "900/900 [==============================] - 1s 1ms/sample - loss: 0.5572 - accuracy: 0.9100 - val_loss: 0.4607 - val_accuracy: 0.9500\n",
      "Epoch 2/100\n",
      "900/900 [==============================] - 0s 102us/sample - loss: 0.4868 - accuracy: 0.9189 - val_loss: 0.4986 - val_accuracy: 0.9400\n",
      "Epoch 3/100\n",
      "900/900 [==============================] - 0s 102us/sample - loss: 0.4185 - accuracy: 0.9267 - val_loss: 0.4676 - val_accuracy: 0.9400\n",
      "Epoch 4/100\n",
      "900/900 [==============================] - 0s 99us/sample - loss: 0.3880 - accuracy: 0.9378 - val_loss: 0.5742 - val_accuracy: 0.9400\n",
      "Epoch 5/100\n",
      "900/900 [==============================] - 0s 99us/sample - loss: 0.4207 - accuracy: 0.9289 - val_loss: 0.5365 - val_accuracy: 0.9500\n",
      "Epoch 6/100\n",
      "900/900 [==============================] - 0s 98us/sample - loss: 0.3750 - accuracy: 0.9444 - val_loss: 0.5380 - val_accuracy: 0.9400\n",
      "Epoch 7/100\n",
      "900/900 [==============================] - 0s 105us/sample - loss: 0.3447 - accuracy: 0.9533 - val_loss: 0.5705 - val_accuracy: 0.9300\n",
      "Epoch 8/100\n",
      "900/900 [==============================] - 0s 106us/sample - loss: 0.3751 - accuracy: 0.9522 - val_loss: 0.5304 - val_accuracy: 0.9300\n",
      "Epoch 9/100\n",
      "900/900 [==============================] - 0s 107us/sample - loss: 0.3427 - accuracy: 0.9511 - val_loss: 0.5533 - val_accuracy: 0.9300\n",
      "Epoch 10/100\n",
      "900/900 [==============================] - 0s 104us/sample - loss: 0.3392 - accuracy: 0.9533 - val_loss: 0.5553 - val_accuracy: 0.9300\n",
      "Epoch 11/100\n",
      "900/900 [==============================] - 0s 101us/sample - loss: 0.3216 - accuracy: 0.9567 - val_loss: 0.5391 - val_accuracy: 0.9400\n",
      "Epoch 12/100\n",
      "900/900 [==============================] - 0s 101us/sample - loss: 0.3357 - accuracy: 0.9556 - val_loss: 0.5759 - val_accuracy: 0.9300\n",
      "Epoch 13/100\n",
      "900/900 [==============================] - 0s 100us/sample - loss: 0.3435 - accuracy: 0.9533 - val_loss: 0.5947 - val_accuracy: 0.9300\n",
      "Epoch 14/100\n",
      "900/900 [==============================] - 0s 103us/sample - loss: 0.3465 - accuracy: 0.9578 - val_loss: 0.5587 - val_accuracy: 0.9400\n",
      "Epoch 15/100\n",
      "900/900 [==============================] - 0s 103us/sample - loss: 0.3329 - accuracy: 0.9600 - val_loss: 0.6329 - val_accuracy: 0.9300\n",
      "Epoch 16/100\n",
      "900/900 [==============================] - 0s 98us/sample - loss: 0.3562 - accuracy: 0.9567 - val_loss: 0.6694 - val_accuracy: 0.9400\n",
      "Epoch 17/100\n",
      "900/900 [==============================] - 0s 106us/sample - loss: 0.3302 - accuracy: 0.9589 - val_loss: 0.6356 - val_accuracy: 0.9300\n",
      "Epoch 18/100\n",
      "900/900 [==============================] - 0s 110us/sample - loss: 0.3487 - accuracy: 0.9533 - val_loss: 0.6193 - val_accuracy: 0.9400\n",
      "Epoch 19/100\n",
      "900/900 [==============================] - 0s 110us/sample - loss: 0.3523 - accuracy: 0.9578 - val_loss: 0.6323 - val_accuracy: 0.9400\n",
      "Epoch 20/100\n",
      "900/900 [==============================] - 0s 113us/sample - loss: 0.3206 - accuracy: 0.9644 - val_loss: 0.6092 - val_accuracy: 0.9200\n",
      "Epoch 21/100\n",
      "900/900 [==============================] - 0s 110us/sample - loss: 0.3243 - accuracy: 0.9622 - val_loss: 0.6513 - val_accuracy: 0.9200\n",
      "Epoch 22/100\n",
      "900/900 [==============================] - 0s 116us/sample - loss: 0.3605 - accuracy: 0.9533 - val_loss: 0.6332 - val_accuracy: 0.9200\n",
      "Epoch 23/100\n",
      "900/900 [==============================] - 0s 112us/sample - loss: 0.3657 - accuracy: 0.9511 - val_loss: 0.5858 - val_accuracy: 0.9300\n",
      "Epoch 24/100\n",
      "900/900 [==============================] - 0s 113us/sample - loss: 0.3266 - accuracy: 0.9622 - val_loss: 0.7028 - val_accuracy: 0.9200\n",
      "Epoch 25/100\n",
      "900/900 [==============================] - 0s 121us/sample - loss: 0.3539 - accuracy: 0.9533 - val_loss: 0.6638 - val_accuracy: 0.9300\n",
      "Epoch 26/100\n",
      "900/900 [==============================] - 0s 115us/sample - loss: 0.3573 - accuracy: 0.9522 - val_loss: 0.6848 - val_accuracy: 0.9200\n",
      "Epoch 27/100\n",
      "900/900 [==============================] - 0s 111us/sample - loss: 0.3910 - accuracy: 0.9467 - val_loss: 0.7006 - val_accuracy: 0.9100\n",
      "Epoch 28/100\n",
      "900/900 [==============================] - 0s 110us/sample - loss: 0.3843 - accuracy: 0.9544 - val_loss: 0.7027 - val_accuracy: 0.9300\n",
      "Epoch 29/100\n",
      "900/900 [==============================] - 0s 113us/sample - loss: 0.3416 - accuracy: 0.9678 - val_loss: 0.6663 - val_accuracy: 0.9300\n",
      "Epoch 30/100\n",
      "900/900 [==============================] - 0s 112us/sample - loss: 0.3576 - accuracy: 0.9656 - val_loss: 0.7069 - val_accuracy: 0.9100\n",
      "Epoch 31/100\n",
      "900/900 [==============================] - 0s 109us/sample - loss: 0.3583 - accuracy: 0.9689 - val_loss: 0.7060 - val_accuracy: 0.9200\n",
      "Epoch 32/100\n",
      "900/900 [==============================] - 0s 109us/sample - loss: 0.3641 - accuracy: 0.9600 - val_loss: 0.7426 - val_accuracy: 0.9200\n",
      "Epoch 33/100\n",
      "900/900 [==============================] - 0s 102us/sample - loss: 0.3769 - accuracy: 0.9511 - val_loss: 0.7440 - val_accuracy: 0.9100\n",
      "Epoch 34/100\n",
      "900/900 [==============================] - 0s 100us/sample - loss: 0.4025 - accuracy: 0.9511 - val_loss: 0.7183 - val_accuracy: 0.9200\n",
      "Epoch 35/100\n",
      "900/900 [==============================] - 0s 97us/sample - loss: 0.3827 - accuracy: 0.9589 - val_loss: 0.7063 - val_accuracy: 0.9100\n",
      "Epoch 36/100\n",
      "900/900 [==============================] - 0s 100us/sample - loss: 0.3709 - accuracy: 0.9644 - val_loss: 0.7141 - val_accuracy: 0.9100\n",
      "Epoch 37/100\n",
      "900/900 [==============================] - 0s 100us/sample - loss: 0.3497 - accuracy: 0.9733 - val_loss: 0.7126 - val_accuracy: 0.9100\n",
      "Epoch 38/100\n",
      "900/900 [==============================] - 0s 99us/sample - loss: 0.3580 - accuracy: 0.9644 - val_loss: 0.6433 - val_accuracy: 0.9100\n",
      "Epoch 39/100\n",
      "900/900 [==============================] - 0s 102us/sample - loss: 0.3707 - accuracy: 0.9589 - val_loss: 0.6623 - val_accuracy: 0.9200\n",
      "Epoch 40/100\n",
      "900/900 [==============================] - 0s 99us/sample - loss: 0.3677 - accuracy: 0.9578 - val_loss: 0.7131 - val_accuracy: 0.9300\n",
      "Epoch 41/100\n",
      "900/900 [==============================] - 0s 98us/sample - loss: 0.3583 - accuracy: 0.9622 - val_loss: 0.7511 - val_accuracy: 0.9300\n",
      "Epoch 42/100\n",
      "900/900 [==============================] - 0s 108us/sample - loss: 0.3534 - accuracy: 0.9622 - val_loss: 0.7183 - val_accuracy: 0.9100\n",
      "Epoch 43/100\n",
      "900/900 [==============================] - 0s 100us/sample - loss: 0.3198 - accuracy: 0.9767 - val_loss: 0.7120 - val_accuracy: 0.9100\n",
      "Epoch 44/100\n",
      "900/900 [==============================] - 0s 108us/sample - loss: 0.3578 - accuracy: 0.9622 - val_loss: 0.7357 - val_accuracy: 0.9100\n",
      "Epoch 45/100\n",
      "900/900 [==============================] - 0s 104us/sample - loss: 0.3414 - accuracy: 0.9633 - val_loss: 0.7499 - val_accuracy: 0.9100\n",
      "Epoch 46/100\n",
      "900/900 [==============================] - 0s 101us/sample - loss: 0.3354 - accuracy: 0.9722 - val_loss: 0.6325 - val_accuracy: 0.9100\n",
      "Epoch 47/100\n",
      "900/900 [==============================] - 0s 100us/sample - loss: 0.3363 - accuracy: 0.9700 - val_loss: 0.6491 - val_accuracy: 0.9400\n",
      "Epoch 48/100\n",
      "900/900 [==============================] - 0s 100us/sample - loss: 0.3265 - accuracy: 0.9722 - val_loss: 0.7315 - val_accuracy: 0.9300\n",
      "Epoch 49/100\n",
      "900/900 [==============================] - 0s 102us/sample - loss: 0.3462 - accuracy: 0.9656 - val_loss: 0.7328 - val_accuracy: 0.9200\n",
      "Epoch 50/100\n",
      "900/900 [==============================] - 0s 99us/sample - loss: 0.3673 - accuracy: 0.9667 - val_loss: 0.6553 - val_accuracy: 0.9400\n",
      "Epoch 51/100\n",
      "900/900 [==============================] - 0s 100us/sample - loss: 0.3666 - accuracy: 0.9622 - val_loss: 0.6978 - val_accuracy: 0.9300\n",
      "Epoch 52/100\n",
      "900/900 [==============================] - 0s 101us/sample - loss: 0.3451 - accuracy: 0.9689 - val_loss: 0.6605 - val_accuracy: 0.9300\n",
      "Epoch 53/100\n",
      "900/900 [==============================] - 0s 100us/sample - loss: 0.3374 - accuracy: 0.9678 - val_loss: 0.7136 - val_accuracy: 0.9400\n",
      "Epoch 54/100\n",
      "900/900 [==============================] - 0s 101us/sample - loss: 0.3668 - accuracy: 0.9589 - val_loss: 0.6234 - val_accuracy: 0.9400\n",
      "Epoch 55/100\n",
      "900/900 [==============================] - 0s 102us/sample - loss: 0.3503 - accuracy: 0.9678 - val_loss: 0.6962 - val_accuracy: 0.9300\n",
      "Epoch 56/100\n",
      "900/900 [==============================] - 0s 103us/sample - loss: 0.3676 - accuracy: 0.9600 - val_loss: 0.6567 - val_accuracy: 0.9300\n",
      "Epoch 57/100\n",
      "900/900 [==============================] - 0s 100us/sample - loss: 0.3341 - accuracy: 0.9733 - val_loss: 0.6681 - val_accuracy: 0.9300\n",
      "Epoch 58/100\n",
      "900/900 [==============================] - 0s 101us/sample - loss: 0.3273 - accuracy: 0.9744 - val_loss: 0.6718 - val_accuracy: 0.9400\n",
      "Epoch 59/100\n",
      "900/900 [==============================] - 0s 106us/sample - loss: 0.3382 - accuracy: 0.9711 - val_loss: 0.6883 - val_accuracy: 0.9300\n",
      "Epoch 60/100\n",
      "900/900 [==============================] - 0s 103us/sample - loss: 0.3263 - accuracy: 0.9756 - val_loss: 0.7615 - val_accuracy: 0.9300\n",
      "Epoch 61/100\n",
      "900/900 [==============================] - 0s 103us/sample - loss: 0.3155 - accuracy: 0.9744 - val_loss: 0.6743 - val_accuracy: 0.9300\n",
      "Epoch 62/100\n",
      "900/900 [==============================] - 0s 101us/sample - loss: 0.3144 - accuracy: 0.9733 - val_loss: 0.7479 - val_accuracy: 0.9200\n",
      "Epoch 63/100\n",
      "900/900 [==============================] - 0s 99us/sample - loss: 0.3093 - accuracy: 0.9689 - val_loss: 0.7168 - val_accuracy: 0.9300\n",
      "Epoch 64/100\n",
      "900/900 [==============================] - 0s 100us/sample - loss: 0.3047 - accuracy: 0.9789 - val_loss: 0.7490 - val_accuracy: 0.9200\n",
      "Epoch 65/100\n",
      "900/900 [==============================] - 0s 100us/sample - loss: 0.3229 - accuracy: 0.9711 - val_loss: 0.6919 - val_accuracy: 0.9200\n",
      "Epoch 66/100\n",
      "900/900 [==============================] - 0s 101us/sample - loss: 0.2918 - accuracy: 0.9844 - val_loss: 0.7335 - val_accuracy: 0.9300\n",
      "Epoch 67/100\n",
      "900/900 [==============================] - 0s 96us/sample - loss: 0.2997 - accuracy: 0.9778 - val_loss: 0.7188 - val_accuracy: 0.9400\n",
      "Epoch 68/100\n",
      "900/900 [==============================] - 0s 101us/sample - loss: 0.3221 - accuracy: 0.9689 - val_loss: 0.6728 - val_accuracy: 0.9300\n",
      "Epoch 69/100\n",
      "900/900 [==============================] - 0s 99us/sample - loss: 0.3039 - accuracy: 0.9756 - val_loss: 0.7271 - val_accuracy: 0.9300\n",
      "Epoch 70/100\n",
      "900/900 [==============================] - 0s 99us/sample - loss: 0.3032 - accuracy: 0.9722 - val_loss: 0.7358 - val_accuracy: 0.9300\n",
      "Epoch 71/100\n",
      "900/900 [==============================] - 0s 101us/sample - loss: 0.3046 - accuracy: 0.9744 - val_loss: 0.6751 - val_accuracy: 0.9200\n",
      "Epoch 72/100\n",
      "900/900 [==============================] - 0s 103us/sample - loss: 0.3088 - accuracy: 0.9733 - val_loss: 0.7129 - val_accuracy: 0.9200\n",
      "Epoch 73/100\n",
      "900/900 [==============================] - 0s 102us/sample - loss: 0.3321 - accuracy: 0.9656 - val_loss: 0.7283 - val_accuracy: 0.9200\n",
      "Epoch 74/100\n",
      "900/900 [==============================] - 0s 101us/sample - loss: 0.3700 - accuracy: 0.9522 - val_loss: 0.7568 - val_accuracy: 0.9100\n",
      "Epoch 75/100\n",
      "900/900 [==============================] - 0s 103us/sample - loss: 0.3856 - accuracy: 0.9489 - val_loss: 0.6666 - val_accuracy: 0.9300\n",
      "Epoch 76/100\n",
      "900/900 [==============================] - 0s 100us/sample - loss: 0.3447 - accuracy: 0.9589 - val_loss: 0.6320 - val_accuracy: 0.9300\n",
      "Epoch 77/100\n",
      "900/900 [==============================] - 0s 107us/sample - loss: 0.3457 - accuracy: 0.9678 - val_loss: 0.7288 - val_accuracy: 0.9200\n",
      "Epoch 78/100\n",
      "900/900 [==============================] - 0s 102us/sample - loss: 0.3450 - accuracy: 0.9722 - val_loss: 0.7449 - val_accuracy: 0.9200\n",
      "Epoch 79/100\n",
      "900/900 [==============================] - 0s 100us/sample - loss: 0.3496 - accuracy: 0.9678 - val_loss: 0.6985 - val_accuracy: 0.9300\n",
      "Epoch 80/100\n",
      "900/900 [==============================] - 0s 101us/sample - loss: 0.3296 - accuracy: 0.9678 - val_loss: 0.6980 - val_accuracy: 0.9200\n",
      "Epoch 81/100\n",
      "900/900 [==============================] - 0s 102us/sample - loss: 0.3236 - accuracy: 0.9744 - val_loss: 0.7248 - val_accuracy: 0.9100\n",
      "Epoch 82/100\n",
      "900/900 [==============================] - 0s 101us/sample - loss: 0.3574 - accuracy: 0.9600 - val_loss: 0.6990 - val_accuracy: 0.9100\n",
      "Epoch 83/100\n",
      "900/900 [==============================] - 0s 99us/sample - loss: 0.3639 - accuracy: 0.9611 - val_loss: 0.6961 - val_accuracy: 0.9300\n",
      "Epoch 84/100\n",
      "900/900 [==============================] - 0s 101us/sample - loss: 0.3197 - accuracy: 0.9733 - val_loss: 0.7553 - val_accuracy: 0.9300\n",
      "Epoch 85/100\n",
      "900/900 [==============================] - 0s 99us/sample - loss: 0.3265 - accuracy: 0.9700 - val_loss: 0.7359 - val_accuracy: 0.9300\n",
      "Epoch 86/100\n",
      "900/900 [==============================] - 0s 98us/sample - loss: 0.3618 - accuracy: 0.9656 - val_loss: 0.7921 - val_accuracy: 0.9100\n",
      "Epoch 87/100\n",
      "900/900 [==============================] - 0s 97us/sample - loss: 0.3283 - accuracy: 0.9744 - val_loss: 0.7123 - val_accuracy: 0.9200\n",
      "Epoch 88/100\n",
      "900/900 [==============================] - 0s 99us/sample - loss: 0.4442 - accuracy: 0.9367 - val_loss: 0.7774 - val_accuracy: 0.9300\n",
      "Epoch 89/100\n",
      "900/900 [==============================] - 0s 100us/sample - loss: 0.3543 - accuracy: 0.9689 - val_loss: 0.6872 - val_accuracy: 0.9400\n",
      "Epoch 90/100\n",
      "900/900 [==============================] - 0s 100us/sample - loss: 0.3599 - accuracy: 0.9656 - val_loss: 0.6802 - val_accuracy: 0.9300\n",
      "Epoch 91/100\n",
      "900/900 [==============================] - 0s 100us/sample - loss: 0.3474 - accuracy: 0.9778 - val_loss: 0.6956 - val_accuracy: 0.9300\n",
      "Epoch 92/100\n",
      "900/900 [==============================] - 0s 99us/sample - loss: 0.3394 - accuracy: 0.9722 - val_loss: 0.7192 - val_accuracy: 0.9300\n",
      "Epoch 93/100\n",
      "900/900 [==============================] - 0s 103us/sample - loss: 0.3586 - accuracy: 0.9667 - val_loss: 0.7090 - val_accuracy: 0.9300\n",
      "Epoch 94/100\n",
      "900/900 [==============================] - 0s 99us/sample - loss: 0.4005 - accuracy: 0.9578 - val_loss: 0.7901 - val_accuracy: 0.9200\n",
      "Epoch 95/100\n",
      "900/900 [==============================] - 0s 106us/sample - loss: 0.4258 - accuracy: 0.9444 - val_loss: 0.8679 - val_accuracy: 0.9200\n",
      "Epoch 96/100\n",
      "900/900 [==============================] - 0s 98us/sample - loss: 0.4292 - accuracy: 0.9478 - val_loss: 0.7419 - val_accuracy: 0.9400\n",
      "Epoch 97/100\n",
      "900/900 [==============================] - 0s 100us/sample - loss: 0.4257 - accuracy: 0.9544 - val_loss: 0.7286 - val_accuracy: 0.9400\n",
      "Epoch 98/100\n",
      "900/900 [==============================] - 0s 98us/sample - loss: 0.4358 - accuracy: 0.9511 - val_loss: 0.7647 - val_accuracy: 0.9300\n",
      "Epoch 99/100\n",
      "900/900 [==============================] - 0s 99us/sample - loss: 0.4462 - accuracy: 0.9533 - val_loss: 0.7591 - val_accuracy: 0.9300\n",
      "Epoch 100/100\n",
      "900/900 [==============================] - 0s 99us/sample - loss: 0.4231 - accuracy: 0.9622 - val_loss: 0.8356 - val_accuracy: 0.9200\n"
     ]
    }
   ],
   "source": [
    "histq= q_aware_model.fit(x_train[:1000], y_train[:1000],\n",
    "                        batch_size=128,\n",
    "                        epochs=100,\n",
    "                        verbose=1,\n",
    "                        validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 111us/sample - loss: 0.4041 - accuracy: 0.9565\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.6350 - accuracy: 0.9052\n",
      "Baseline test accuracy: 95.6499993801117\n",
      "Quant test accuracy: 90.52000045776367\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    x_test, y_test, verbose=1)\n",
    "\n",
    "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
    "    x_test, y_test, verbose=1)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy*100)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize some layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "annotate = tfmot.quantization.keras.quantize_annotate_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_3 (Dropout)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              785000    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_1 (Quantiz (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 1,796,010\n",
      "Trainable params: 1,796,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import models, layers\n",
    "from keras import regularizers\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dropout(0.2,input_shape=(784,)))\n",
    "model.add(keras.layers.Dense(1000,\n",
    "                        kernel_regularizer = regularizers.l2(0.01),\n",
    "                        activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(annotate(keras.layers.Dense(1000,\n",
    "                        kernel_regularizer = regularizers.l2(0.01),\n",
    "                        activation='relu')))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(10,  activation='softmax'))\n",
    "#display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f84f811ecd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f84f811ecd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_3 (Dropout)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              785000    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "quant_dense_4 (QuantizeWrapp (None, 1000)              1001005   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 1,796,015\n",
      "Trainable params: 1,796,010\n",
      "Non-trainable params: 5\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Use `quantize_apply` to actually make the model quantization aware.\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_aware_model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/100\n",
      "54000/54000 [==============================] - 4s 68us/sample - loss: 2.1874 - accuracy: 0.8525 - val_loss: 0.6378 - val_accuracy: 0.9402\n",
      "Epoch 2/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.7794 - accuracy: 0.8882 - val_loss: 0.6252 - val_accuracy: 0.9367\n",
      "Epoch 3/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.7430 - accuracy: 0.8960 - val_loss: 0.6064 - val_accuracy: 0.9448\n",
      "Epoch 4/100\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.7241 - accuracy: 0.8996 - val_loss: 0.5572 - val_accuracy: 0.9543\n",
      "Epoch 5/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.7001 - accuracy: 0.9026 - val_loss: 0.5618 - val_accuracy: 0.9490\n",
      "Epoch 6/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.6919 - accuracy: 0.9021 - val_loss: 0.5340 - val_accuracy: 0.9577\n",
      "Epoch 7/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.6875 - accuracy: 0.9035 - val_loss: 0.5293 - val_accuracy: 0.9593\n",
      "Epoch 8/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.6760 - accuracy: 0.9042 - val_loss: 0.5166 - val_accuracy: 0.9590\n",
      "Epoch 9/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.6551 - accuracy: 0.9075 - val_loss: 0.4881 - val_accuracy: 0.9613\n",
      "Epoch 10/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.6441 - accuracy: 0.9096 - val_loss: 0.4837 - val_accuracy: 0.9623\n",
      "Epoch 11/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.6386 - accuracy: 0.9080 - val_loss: 0.4879 - val_accuracy: 0.9562\n",
      "Epoch 12/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.6340 - accuracy: 0.9104 - val_loss: 0.4600 - val_accuracy: 0.9663\n",
      "Epoch 13/100\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.6331 - accuracy: 0.9096 - val_loss: 0.4815 - val_accuracy: 0.9582\n",
      "Epoch 14/100\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.6266 - accuracy: 0.9082 - val_loss: 0.4573 - val_accuracy: 0.9642\n",
      "Epoch 15/100\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.6189 - accuracy: 0.9087 - val_loss: 0.4541 - val_accuracy: 0.9638\n",
      "Epoch 16/100\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.6187 - accuracy: 0.9100 - val_loss: 0.4672 - val_accuracy: 0.9600\n",
      "Epoch 17/100\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.6163 - accuracy: 0.9094 - val_loss: 0.4574 - val_accuracy: 0.9588\n",
      "Epoch 18/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.6100 - accuracy: 0.9104 - val_loss: 0.4518 - val_accuracy: 0.9622\n",
      "Epoch 19/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.6090 - accuracy: 0.9085 - val_loss: 0.4619 - val_accuracy: 0.9522\n",
      "Epoch 20/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.6091 - accuracy: 0.9085 - val_loss: 0.4718 - val_accuracy: 0.9575\n",
      "Epoch 21/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.6060 - accuracy: 0.9088 - val_loss: 0.4670 - val_accuracy: 0.9578\n",
      "Epoch 22/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.6060 - accuracy: 0.9101 - val_loss: 0.4413 - val_accuracy: 0.9627\n",
      "Epoch 23/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5965 - accuracy: 0.9113 - val_loss: 0.4530 - val_accuracy: 0.9595\n",
      "Epoch 24/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.6002 - accuracy: 0.9105 - val_loss: 0.4382 - val_accuracy: 0.9630\n",
      "Epoch 25/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.6012 - accuracy: 0.9096 - val_loss: 0.4337 - val_accuracy: 0.9592\n",
      "Epoch 26/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5925 - accuracy: 0.9107 - val_loss: 0.4458 - val_accuracy: 0.9568\n",
      "Epoch 27/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5951 - accuracy: 0.9096 - val_loss: 0.4419 - val_accuracy: 0.9610\n",
      "Epoch 28/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5953 - accuracy: 0.9103 - val_loss: 0.4330 - val_accuracy: 0.9650\n",
      "Epoch 29/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5865 - accuracy: 0.9100 - val_loss: 0.4254 - val_accuracy: 0.9653\n",
      "Epoch 30/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5935 - accuracy: 0.9092 - val_loss: 0.4208 - val_accuracy: 0.9610\n",
      "Epoch 31/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5902 - accuracy: 0.9086 - val_loss: 0.4372 - val_accuracy: 0.9623\n",
      "Epoch 32/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5932 - accuracy: 0.9080 - val_loss: 0.4259 - val_accuracy: 0.9630\n",
      "Epoch 33/100\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.5837 - accuracy: 0.9099 - val_loss: 0.4286 - val_accuracy: 0.9635\n",
      "Epoch 34/100\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.5792 - accuracy: 0.9109 - val_loss: 0.4190 - val_accuracy: 0.9613\n",
      "Epoch 35/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5773 - accuracy: 0.9120 - val_loss: 0.4135 - val_accuracy: 0.9632\n",
      "Epoch 36/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5828 - accuracy: 0.9098 - val_loss: 0.4256 - val_accuracy: 0.9630\n",
      "Epoch 37/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5716 - accuracy: 0.9122 - val_loss: 0.4114 - val_accuracy: 0.9635\n",
      "Epoch 38/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5793 - accuracy: 0.9106 - val_loss: 0.4081 - val_accuracy: 0.9700\n",
      "Epoch 39/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5758 - accuracy: 0.9117 - val_loss: 0.4135 - val_accuracy: 0.9627\n",
      "Epoch 40/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5786 - accuracy: 0.9100 - val_loss: 0.4058 - val_accuracy: 0.9647\n",
      "Epoch 41/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5719 - accuracy: 0.9110 - val_loss: 0.4023 - val_accuracy: 0.9685\n",
      "Epoch 42/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5755 - accuracy: 0.9098 - val_loss: 0.4095 - val_accuracy: 0.9650\n",
      "Epoch 43/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5747 - accuracy: 0.9105 - val_loss: 0.4008 - val_accuracy: 0.9660\n",
      "Epoch 44/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5659 - accuracy: 0.9123 - val_loss: 0.3912 - val_accuracy: 0.9667\n",
      "Epoch 45/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5652 - accuracy: 0.9126 - val_loss: 0.3879 - val_accuracy: 0.9682\n",
      "Epoch 46/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5742 - accuracy: 0.9086 - val_loss: 0.4000 - val_accuracy: 0.9620\n",
      "Epoch 47/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5742 - accuracy: 0.9090 - val_loss: 0.4134 - val_accuracy: 0.9567\n",
      "Epoch 48/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5683 - accuracy: 0.9102 - val_loss: 0.3982 - val_accuracy: 0.9655\n",
      "Epoch 49/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5677 - accuracy: 0.9102 - val_loss: 0.3979 - val_accuracy: 0.9658\n",
      "Epoch 50/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5651 - accuracy: 0.9093 - val_loss: 0.4109 - val_accuracy: 0.9645\n",
      "Epoch 51/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5636 - accuracy: 0.9094 - val_loss: 0.3882 - val_accuracy: 0.9650\n",
      "Epoch 52/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5659 - accuracy: 0.9109 - val_loss: 0.3931 - val_accuracy: 0.9667\n",
      "Epoch 53/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5682 - accuracy: 0.9105 - val_loss: 0.3901 - val_accuracy: 0.9672\n",
      "Epoch 54/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5618 - accuracy: 0.9098 - val_loss: 0.3883 - val_accuracy: 0.9668\n",
      "Epoch 55/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5666 - accuracy: 0.9088 - val_loss: 0.3952 - val_accuracy: 0.9655\n",
      "Epoch 56/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5639 - accuracy: 0.9103 - val_loss: 0.3797 - val_accuracy: 0.9665\n",
      "Epoch 57/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5619 - accuracy: 0.9098 - val_loss: 0.3900 - val_accuracy: 0.9630\n",
      "Epoch 58/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5684 - accuracy: 0.9096 - val_loss: 0.3990 - val_accuracy: 0.9617\n",
      "Epoch 59/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5547 - accuracy: 0.9110 - val_loss: 0.4025 - val_accuracy: 0.9642\n",
      "Epoch 60/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5629 - accuracy: 0.9101 - val_loss: 0.4004 - val_accuracy: 0.9625\n",
      "Epoch 61/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5619 - accuracy: 0.9110 - val_loss: 0.3842 - val_accuracy: 0.9652\n",
      "Epoch 62/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5593 - accuracy: 0.9111 - val_loss: 0.3914 - val_accuracy: 0.9652\n",
      "Epoch 63/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5641 - accuracy: 0.9104 - val_loss: 0.4004 - val_accuracy: 0.9570\n",
      "Epoch 64/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5604 - accuracy: 0.9101 - val_loss: 0.3834 - val_accuracy: 0.9682\n",
      "Epoch 65/100\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.5611 - accuracy: 0.9093 - val_loss: 0.3845 - val_accuracy: 0.9675\n",
      "Epoch 66/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5563 - accuracy: 0.9115 - val_loss: 0.3835 - val_accuracy: 0.9653\n",
      "Epoch 67/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5609 - accuracy: 0.9095 - val_loss: 0.3828 - val_accuracy: 0.9643\n",
      "Epoch 68/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5576 - accuracy: 0.9104 - val_loss: 0.3908 - val_accuracy: 0.9663\n",
      "Epoch 69/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5542 - accuracy: 0.9116 - val_loss: 0.3851 - val_accuracy: 0.9663\n",
      "Epoch 70/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5567 - accuracy: 0.9110 - val_loss: 0.3798 - val_accuracy: 0.9675\n",
      "Epoch 71/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5604 - accuracy: 0.9095 - val_loss: 0.3832 - val_accuracy: 0.9667\n",
      "Epoch 72/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5662 - accuracy: 0.9088 - val_loss: 0.3906 - val_accuracy: 0.9655\n",
      "Epoch 73/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5531 - accuracy: 0.9104 - val_loss: 0.3895 - val_accuracy: 0.9628\n",
      "Epoch 74/100\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.5598 - accuracy: 0.9085 - val_loss: 0.3821 - val_accuracy: 0.9672\n",
      "Epoch 75/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5661 - accuracy: 0.9078 - val_loss: 0.3939 - val_accuracy: 0.9622\n",
      "Epoch 76/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5592 - accuracy: 0.9095 - val_loss: 0.3869 - val_accuracy: 0.9627\n",
      "Epoch 77/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5593 - accuracy: 0.9102 - val_loss: 0.3777 - val_accuracy: 0.9667\n",
      "Epoch 78/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5570 - accuracy: 0.9080 - val_loss: 0.3968 - val_accuracy: 0.9632\n",
      "Epoch 79/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5561 - accuracy: 0.9112 - val_loss: 0.3829 - val_accuracy: 0.9653\n",
      "Epoch 80/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5572 - accuracy: 0.9104 - val_loss: 0.3927 - val_accuracy: 0.9618\n",
      "Epoch 81/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5584 - accuracy: 0.9101 - val_loss: 0.3823 - val_accuracy: 0.9660\n",
      "Epoch 82/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5539 - accuracy: 0.9108 - val_loss: 0.3845 - val_accuracy: 0.9648\n",
      "Epoch 83/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5577 - accuracy: 0.9093 - val_loss: 0.3829 - val_accuracy: 0.9658\n",
      "Epoch 84/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5563 - accuracy: 0.9103 - val_loss: 0.3799 - val_accuracy: 0.9672\n",
      "Epoch 85/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5598 - accuracy: 0.9076 - val_loss: 0.3869 - val_accuracy: 0.9650\n",
      "Epoch 86/100\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5567 - accuracy: 0.9106 - val_loss: 0.3726 - val_accuracy: 0.9688\n",
      "Epoch 87/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5578 - accuracy: 0.9090 - val_loss: 0.3789 - val_accuracy: 0.9683\n",
      "Epoch 88/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5494 - accuracy: 0.9106 - val_loss: 0.3827 - val_accuracy: 0.9648\n",
      "Epoch 89/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5535 - accuracy: 0.9094 - val_loss: 0.3809 - val_accuracy: 0.9648\n",
      "Epoch 90/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5524 - accuracy: 0.9099 - val_loss: 0.3803 - val_accuracy: 0.9617\n",
      "Epoch 91/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5472 - accuracy: 0.9111 - val_loss: 0.3737 - val_accuracy: 0.9662\n",
      "Epoch 92/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5494 - accuracy: 0.9102 - val_loss: 0.3788 - val_accuracy: 0.9665\n",
      "Epoch 93/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5611 - accuracy: 0.9078 - val_loss: 0.3750 - val_accuracy: 0.9650\n",
      "Epoch 94/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5446 - accuracy: 0.9118 - val_loss: 0.3641 - val_accuracy: 0.9707\n",
      "Epoch 95/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5490 - accuracy: 0.9094 - val_loss: 0.3781 - val_accuracy: 0.9628\n",
      "Epoch 96/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5566 - accuracy: 0.9079 - val_loss: 0.3754 - val_accuracy: 0.9682\n",
      "Epoch 97/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5526 - accuracy: 0.9102 - val_loss: 0.3744 - val_accuracy: 0.9633\n",
      "Epoch 98/100\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5514 - accuracy: 0.9083 - val_loss: 0.3648 - val_accuracy: 0.9675\n",
      "Epoch 99/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5529 - accuracy: 0.9087 - val_loss: 0.3844 - val_accuracy: 0.9658\n",
      "Epoch 100/100\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.5500 - accuracy: 0.9103 - val_loss: 0.3723 - val_accuracy: 0.9672\n"
     ]
    }
   ],
   "source": [
    "histq = quant_aware_model.fit(x_train, y_train,\n",
    "                        batch_size=128,\n",
    "                        epochs=100,\n",
    "                        verbose=1,\n",
    "                        validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 85us/sample - loss: 0.4013 - accuracy: 0.9563\n",
      "Quant test accuracy: 95.63000202178955\n"
     ]
    }
   ],
   "source": [
    "_, quant_aware_model_accuracy = quant_aware_model.evaluate(\n",
    "    x_test, y_test, verbose=1)\n",
    "\n",
    "print('Quant test accuracy:', quant_aware_model_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "(x_train,y_train),(x_test,y_test)=cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 1, 1, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 15,250,250\n",
      "Trainable params: 535,562\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "vgg_model = tf.keras.applications.VGG16(input_shape=x_train[0].shape, include_top=False, weights=None)\n",
    "vgg_model.trainable = True\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(vgg_model)\n",
    "model.add(global_average_layer)\n",
    "model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "#model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses\n",
    "#sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer='adam', \n",
    "              loss=losses.sparse_categorical_crossentropy,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "45000/45000 [==============================] - 20s 449us/sample - loss: 2.0840 - accuracy: 0.2348 - val_loss: 1.9510 - val_accuracy: 0.2824\n",
      "Epoch 2/20\n",
      "45000/45000 [==============================] - 14s 314us/sample - loss: 1.8850 - accuracy: 0.3179 - val_loss: 1.8595 - val_accuracy: 0.3204\n",
      "Epoch 3/20\n",
      "45000/45000 [==============================] - 14s 315us/sample - loss: 1.8158 - accuracy: 0.3437 - val_loss: 1.7738 - val_accuracy: 0.3648\n",
      "Epoch 4/20\n",
      "45000/45000 [==============================] - 14s 317us/sample - loss: 1.7637 - accuracy: 0.3640 - val_loss: 1.7309 - val_accuracy: 0.3668\n",
      "Epoch 5/20\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.7258 - accuracy: 0.3780 - val_loss: 1.7263 - val_accuracy: 0.3758\n",
      "Epoch 6/20\n",
      "45000/45000 [==============================] - 14s 321us/sample - loss: 1.7037 - accuracy: 0.3912 - val_loss: 1.6905 - val_accuracy: 0.3842\n",
      "Epoch 7/20\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.6866 - accuracy: 0.3941 - val_loss: 1.6837 - val_accuracy: 0.3972\n",
      "Epoch 8/20\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.6610 - accuracy: 0.4084 - val_loss: 1.6823 - val_accuracy: 0.3884\n",
      "Epoch 9/20\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.6500 - accuracy: 0.4084 - val_loss: 1.6448 - val_accuracy: 0.4096\n",
      "Epoch 10/20\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.6428 - accuracy: 0.4128 - val_loss: 1.6324 - val_accuracy: 0.4118\n",
      "Epoch 11/20\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.6287 - accuracy: 0.4177 - val_loss: 1.6355 - val_accuracy: 0.4062\n",
      "Epoch 12/20\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.6132 - accuracy: 0.4238 - val_loss: 1.6185 - val_accuracy: 0.4184\n",
      "Epoch 13/20\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.6111 - accuracy: 0.4257 - val_loss: 1.5836 - val_accuracy: 0.4304\n",
      "Epoch 14/20\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.6007 - accuracy: 0.4300 - val_loss: 1.6019 - val_accuracy: 0.4240\n",
      "Epoch 15/20\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.5864 - accuracy: 0.4356 - val_loss: 1.6184 - val_accuracy: 0.4156\n",
      "Epoch 16/20\n",
      "45000/45000 [==============================] - 14s 320us/sample - loss: 1.5880 - accuracy: 0.4340 - val_loss: 1.6168 - val_accuracy: 0.4116\n",
      "Epoch 17/20\n",
      "45000/45000 [==============================] - 14s 321us/sample - loss: 1.5764 - accuracy: 0.4369 - val_loss: 1.6033 - val_accuracy: 0.4158\n",
      "Epoch 18/20\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.5690 - accuracy: 0.4411 - val_loss: 1.5759 - val_accuracy: 0.4254\n",
      "Epoch 19/20\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.5639 - accuracy: 0.4425 - val_loss: 1.5683 - val_accuracy: 0.4334\n",
      "Epoch 20/20\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.5599 - accuracy: 0.4452 - val_loss: 1.5582 - val_accuracy: 0.4434\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train,\n",
    "                        batch_size=128,\n",
    "                        epochs=20,\n",
    "                        verbose=1,\n",
    "                        validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss 1.5641, accuracy 44.35%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss {:.4f}, accuracy {:.2f}%\".format(score[0], score[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save(\"vgg_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "45000/45000 [==============================] - 15s 337us/sample - loss: 1.5492 - accuracy: 0.4503 - val_loss: 1.5598 - val_accuracy: 0.4420\n",
      "Epoch 2/50\n",
      "45000/45000 [==============================] - 14s 317us/sample - loss: 1.5499 - accuracy: 0.4491 - val_loss: 1.5494 - val_accuracy: 0.4464\n",
      "Epoch 3/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.5463 - accuracy: 0.4498 - val_loss: 1.5951 - val_accuracy: 0.4350\n",
      "Epoch 4/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.5398 - accuracy: 0.4518 - val_loss: 1.5661 - val_accuracy: 0.4380\n",
      "Epoch 5/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.5320 - accuracy: 0.4558 - val_loss: 1.5503 - val_accuracy: 0.4480\n",
      "Epoch 6/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.5297 - accuracy: 0.4565 - val_loss: 1.5489 - val_accuracy: 0.4376\n",
      "Epoch 7/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.5250 - accuracy: 0.4580 - val_loss: 1.5951 - val_accuracy: 0.4386\n",
      "Epoch 8/50\n",
      "45000/45000 [==============================] - 14s 320us/sample - loss: 1.5241 - accuracy: 0.4594 - val_loss: 1.5400 - val_accuracy: 0.4478\n",
      "Epoch 9/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.5138 - accuracy: 0.4648 - val_loss: 1.5195 - val_accuracy: 0.4656\n",
      "Epoch 10/50\n",
      "45000/45000 [==============================] - 14s 321us/sample - loss: 1.5140 - accuracy: 0.4628 - val_loss: 1.5690 - val_accuracy: 0.4400\n",
      "Epoch 11/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.5032 - accuracy: 0.4648 - val_loss: 1.5445 - val_accuracy: 0.4472\n",
      "Epoch 12/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.5029 - accuracy: 0.4654 - val_loss: 1.5314 - val_accuracy: 0.4526\n",
      "Epoch 13/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.5025 - accuracy: 0.4652 - val_loss: 1.5092 - val_accuracy: 0.4602\n",
      "Epoch 14/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.4946 - accuracy: 0.4696 - val_loss: 1.5011 - val_accuracy: 0.4606\n",
      "Epoch 15/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.5007 - accuracy: 0.4652 - val_loss: 1.5596 - val_accuracy: 0.4394\n",
      "Epoch 16/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.4909 - accuracy: 0.4703 - val_loss: 1.5042 - val_accuracy: 0.4584\n",
      "Epoch 17/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.4863 - accuracy: 0.4692 - val_loss: 1.5428 - val_accuracy: 0.4386\n",
      "Epoch 18/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.4793 - accuracy: 0.4720 - val_loss: 1.5064 - val_accuracy: 0.4610\n",
      "Epoch 19/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.4800 - accuracy: 0.4729 - val_loss: 1.4923 - val_accuracy: 0.4666\n",
      "Epoch 20/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.4758 - accuracy: 0.4758 - val_loss: 1.4981 - val_accuracy: 0.4696\n",
      "Epoch 21/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.4755 - accuracy: 0.4753 - val_loss: 1.5326 - val_accuracy: 0.4512\n",
      "Epoch 22/50\n",
      "45000/45000 [==============================] - 14s 321us/sample - loss: 1.4696 - accuracy: 0.4785 - val_loss: 1.5115 - val_accuracy: 0.4554\n",
      "Epoch 23/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.4704 - accuracy: 0.4770 - val_loss: 1.5147 - val_accuracy: 0.4562\n",
      "Epoch 24/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.4579 - accuracy: 0.4836 - val_loss: 1.5279 - val_accuracy: 0.4508\n",
      "Epoch 25/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.4605 - accuracy: 0.4798 - val_loss: 1.5109 - val_accuracy: 0.4584\n",
      "Epoch 26/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.4564 - accuracy: 0.4813 - val_loss: 1.5005 - val_accuracy: 0.4654\n",
      "Epoch 27/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.4517 - accuracy: 0.4846 - val_loss: 1.5119 - val_accuracy: 0.4620\n",
      "Epoch 28/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.4484 - accuracy: 0.4869 - val_loss: 1.4984 - val_accuracy: 0.4692\n",
      "Epoch 29/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.4482 - accuracy: 0.4843 - val_loss: 1.4918 - val_accuracy: 0.4644\n",
      "Epoch 30/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.4415 - accuracy: 0.4883 - val_loss: 1.5053 - val_accuracy: 0.4508\n",
      "Epoch 31/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.4438 - accuracy: 0.4868 - val_loss: 1.4842 - val_accuracy: 0.4716\n",
      "Epoch 32/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.4392 - accuracy: 0.4888 - val_loss: 1.4775 - val_accuracy: 0.4700\n",
      "Epoch 33/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.4349 - accuracy: 0.4918 - val_loss: 1.4679 - val_accuracy: 0.4790\n",
      "Epoch 34/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.4348 - accuracy: 0.4894 - val_loss: 1.4883 - val_accuracy: 0.4582\n",
      "Epoch 35/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.4324 - accuracy: 0.4898 - val_loss: 1.5084 - val_accuracy: 0.4542\n",
      "Epoch 36/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.4236 - accuracy: 0.4951 - val_loss: 1.4654 - val_accuracy: 0.4700\n",
      "Epoch 37/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.4277 - accuracy: 0.4930 - val_loss: 1.5412 - val_accuracy: 0.4474\n",
      "Epoch 38/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.4243 - accuracy: 0.4927 - val_loss: 1.4985 - val_accuracy: 0.4630\n",
      "Epoch 39/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.4228 - accuracy: 0.4928 - val_loss: 1.5000 - val_accuracy: 0.4662\n",
      "Epoch 40/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.4189 - accuracy: 0.4972 - val_loss: 1.4737 - val_accuracy: 0.4748\n",
      "Epoch 41/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.4122 - accuracy: 0.4972 - val_loss: 1.4714 - val_accuracy: 0.4762\n",
      "Epoch 42/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.4159 - accuracy: 0.4947 - val_loss: 1.4728 - val_accuracy: 0.4750\n",
      "Epoch 43/50\n",
      "45000/45000 [==============================] - 14s 319us/sample - loss: 1.4083 - accuracy: 0.5008 - val_loss: 1.4666 - val_accuracy: 0.4758\n",
      "Epoch 44/50\n",
      "45000/45000 [==============================] - 14s 321us/sample - loss: 1.4042 - accuracy: 0.5022 - val_loss: 1.4574 - val_accuracy: 0.4724\n",
      "Epoch 45/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.4023 - accuracy: 0.5029 - val_loss: 1.4936 - val_accuracy: 0.4644\n",
      "Epoch 46/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.4019 - accuracy: 0.5004 - val_loss: 1.4934 - val_accuracy: 0.4724\n",
      "Epoch 47/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.3993 - accuracy: 0.5012 - val_loss: 1.4533 - val_accuracy: 0.4780\n",
      "Epoch 48/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.3952 - accuracy: 0.5044 - val_loss: 1.5138 - val_accuracy: 0.4566\n",
      "Epoch 49/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.4009 - accuracy: 0.5018 - val_loss: 1.4755 - val_accuracy: 0.4678\n",
      "Epoch 50/50\n",
      "45000/45000 [==============================] - 14s 318us/sample - loss: 1.3887 - accuracy: 0.5064 - val_loss: 1.4799 - val_accuracy: 0.4784\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('vgg_model.h5')\n",
    "model.compile(optimizer='adam', \n",
    "              loss=losses.sparse_categorical_crossentropy,\n",
    "              metrics=[\"accuracy\"])\n",
    "hist = model.fit(x_train, y_train,\n",
    "                        batch_size=128,\n",
    "                        epochs=50,\n",
    "                        verbose=1,\n",
    "                        validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15259960"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('vgg_model.h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "open(\"vgg_quant_model.tflite\", \"wb\").write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.64\n"
     ]
    }
   ],
   "source": [
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"vgg_quant_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test model on some input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "acc=0\n",
    "for i in range(len(x_test)):\n",
    "    input_data = x_test[i].reshape(input_shape)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    if(np.argmax(output_data) == y_test[i]):\n",
    "        acc+=1\n",
    "acc = acc/len(x_test)\n",
    "print(acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model in Mb: 58.182395935058594\n",
      "Quantized model in Mb: 14.566337585449219\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Create float TFLite model.\n",
    "float_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "float_tflite_model = float_converter.convert()\n",
    "\n",
    "# Measure sizes of models.\n",
    "_, float_file = tempfile.mkstemp('.tflite')\n",
    "_, quant_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(quant_file, 'wb') as f:\n",
    "  f.write(tflite_quant_model)\n",
    "\n",
    "with open(float_file, 'wb') as f:\n",
    "  f.write(float_tflite_model)\n",
    "\n",
    "print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 15,250,250\n",
      "Trainable params: 15,250,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "vgg_model = tf.keras.applications.VGG16(input_shape=(32, 32, 3), include_top=False, weights=None)\n",
    "vgg_model.trainable = True\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "#https://github.com/tensorflow/model-optimization/issues/40    \n",
    "model = tf.keras.Sequential()\n",
    "#Had to add input layer, otherwise ran into trouble when printing model summary as well as converting to tf-lite\n",
    "model.add(layers.Input(shape=(32, 32, 3)))\n",
    "input_layer = True\n",
    "for layer in vgg_model.layers:\n",
    "    model.add(layer)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "#model.add(layers.BatchNormalization())\n",
    "#model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "45000/45000 [==============================] - 42s 944us/sample - loss: 2.3878 - accuracy: 0.1385 - val_loss: 1.9351 - val_accuracy: 0.2294\n",
      "Epoch 2/20\n",
      "45000/45000 [==============================] - 41s 922us/sample - loss: 1.7715 - accuracy: 0.2946 - val_loss: 1.6209 - val_accuracy: 0.3746\n",
      "Epoch 3/20\n",
      "45000/45000 [==============================] - 42s 922us/sample - loss: 1.5206 - accuracy: 0.4189 - val_loss: 1.3773 - val_accuracy: 0.4782\n",
      "Epoch 4/20\n",
      "45000/45000 [==============================] - 42s 923us/sample - loss: 1.2979 - accuracy: 0.5241 - val_loss: 1.1892 - val_accuracy: 0.5740\n",
      "Epoch 5/20\n",
      "45000/45000 [==============================] - 42s 923us/sample - loss: 1.1204 - accuracy: 0.6008 - val_loss: 1.0787 - val_accuracy: 0.6208\n",
      "Epoch 6/20\n",
      "45000/45000 [==============================] - 41s 922us/sample - loss: 0.9860 - accuracy: 0.6480 - val_loss: 0.9629 - val_accuracy: 0.6546\n",
      "Epoch 7/20\n",
      "45000/45000 [==============================] - 42s 922us/sample - loss: 0.8621 - accuracy: 0.6990 - val_loss: 0.8815 - val_accuracy: 0.7010\n",
      "Epoch 8/20\n",
      "45000/45000 [==============================] - 42s 923us/sample - loss: 0.7707 - accuracy: 0.7306 - val_loss: 0.8762 - val_accuracy: 0.7010\n",
      "Epoch 9/20\n",
      "45000/45000 [==============================] - 42s 923us/sample - loss: 0.6762 - accuracy: 0.7687 - val_loss: 0.8663 - val_accuracy: 0.7148\n",
      "Epoch 10/20\n",
      "45000/45000 [==============================] - 42s 923us/sample - loss: 0.6022 - accuracy: 0.7920 - val_loss: 0.8997 - val_accuracy: 0.7130\n",
      "Epoch 11/20\n",
      "45000/45000 [==============================] - 41s 921us/sample - loss: 0.5257 - accuracy: 0.8210 - val_loss: 0.8897 - val_accuracy: 0.7180\n",
      "Epoch 12/20\n",
      "45000/45000 [==============================] - 42s 922us/sample - loss: 0.4758 - accuracy: 0.8386 - val_loss: 0.8652 - val_accuracy: 0.7386\n",
      "Epoch 13/20\n",
      "45000/45000 [==============================] - 41s 922us/sample - loss: 0.4347 - accuracy: 0.8525 - val_loss: 0.9065 - val_accuracy: 0.7152\n",
      "Epoch 14/20\n",
      "45000/45000 [==============================] - 41s 922us/sample - loss: 0.3815 - accuracy: 0.8727 - val_loss: 0.9291 - val_accuracy: 0.7380\n",
      "Epoch 15/20\n",
      "45000/45000 [==============================] - 41s 921us/sample - loss: 0.3389 - accuracy: 0.8883 - val_loss: 0.8705 - val_accuracy: 0.7428\n",
      "Epoch 16/20\n",
      "45000/45000 [==============================] - 42s 922us/sample - loss: 0.2976 - accuracy: 0.9014 - val_loss: 0.9502 - val_accuracy: 0.7412\n",
      "Epoch 17/20\n",
      "45000/45000 [==============================] - 42s 923us/sample - loss: 0.2707 - accuracy: 0.9112 - val_loss: 0.9633 - val_accuracy: 0.7506\n",
      "Epoch 18/20\n",
      "45000/45000 [==============================] - 41s 922us/sample - loss: 0.2447 - accuracy: 0.9200 - val_loss: 0.9223 - val_accuracy: 0.7564\n",
      "Epoch 19/20\n",
      "45000/45000 [==============================] - 41s 922us/sample - loss: 0.2446 - accuracy: 0.9215 - val_loss: 1.0310 - val_accuracy: 0.7596\n",
      "Epoch 20/20\n",
      "45000/45000 [==============================] - 41s 922us/sample - loss: 0.1975 - accuracy: 0.9376 - val_loss: 1.0630 - val_accuracy: 0.7506\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=losses.sparse_categorical_crossentropy, \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "hist= model.fit(x_train, y_train,\n",
    "                        batch_size=128,\n",
    "                        epochs=20,\n",
    "                        verbose=1,\n",
    "                        validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save(\"vgg_true_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7fb1936d7390>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7fb1936d7390>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7fb193666810>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7fb193666810>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7fb193666e50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7fb193666e50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7fb193674950>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7fb193674950>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7fb19373a0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7fb193680450>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7fb193680450>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7fb193680790>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7fb193680790>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7fb193680b10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7fb193680b10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7fb193680e90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7fb193680e90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quant_block1_conv1 (Quantize (None, 32, 32, 64)        1923      \n",
      "_________________________________________________________________\n",
      "quant_block1_conv2 (Quantize (None, 32, 32, 64)        37059     \n",
      "_________________________________________________________________\n",
      "quant_block1_pool (QuantizeW (None, 16, 16, 64)        1         \n",
      "_________________________________________________________________\n",
      "quant_block2_conv1 (Quantize (None, 16, 16, 128)       74115     \n",
      "_________________________________________________________________\n",
      "quant_block2_conv2 (Quantize (None, 16, 16, 128)       147843    \n",
      "_________________________________________________________________\n",
      "quant_block2_pool (QuantizeW (None, 8, 8, 128)         1         \n",
      "_________________________________________________________________\n",
      "quant_block3_conv1 (Quantize (None, 8, 8, 256)         295683    \n",
      "_________________________________________________________________\n",
      "quant_block3_conv2 (Quantize (None, 8, 8, 256)         590595    \n",
      "_________________________________________________________________\n",
      "quant_block3_conv3 (Quantize (None, 8, 8, 256)         590595    \n",
      "_________________________________________________________________\n",
      "quant_block3_pool (QuantizeW (None, 4, 4, 256)         1         \n",
      "_________________________________________________________________\n",
      "quant_block4_conv1 (Quantize (None, 4, 4, 512)         1181187   \n",
      "_________________________________________________________________\n",
      "quant_block4_conv2 (Quantize (None, 4, 4, 512)         2360835   \n",
      "_________________________________________________________________\n",
      "quant_block4_conv3 (Quantize (None, 4, 4, 512)         2360835   \n",
      "_________________________________________________________________\n",
      "quant_block4_pool (QuantizeW (None, 2, 2, 512)         1         \n",
      "_________________________________________________________________\n",
      "quant_block5_conv1 (Quantize (None, 2, 2, 512)         2360835   \n",
      "_________________________________________________________________\n",
      "quant_block5_conv2 (Quantize (None, 2, 2, 512)         2360835   \n",
      "_________________________________________________________________\n",
      "quant_block5_conv3 (Quantize (None, 2, 2, 512)         2360835   \n",
      "_________________________________________________________________\n",
      "quant_block5_pool (QuantizeW (None, 1, 1, 512)         1         \n",
      "_________________________________________________________________\n",
      "quant_flatten_6 (QuantizeWra (None, 512)               1         \n",
      "_________________________________________________________________\n",
      "quant_dense_14 (QuantizeWrap (None, 1024)              525317    \n",
      "_________________________________________________________________\n",
      "quant_dense_15 (QuantizeWrap (None, 10)                10255     \n",
      "=================================================================\n",
      "Total params: 15,258,753\n",
      "Trainable params: 15,250,250\n",
      "Non-trainable params: 8,503\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "#model = tf.keras.models.load_model('vgg_model.h5')\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# q_aware stands for for quantization aware.\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(loss=losses.sparse_categorical_crossentropy, \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=losses.sparse_categorical_crossentropy, \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 900 samples, validate on 100 samples\n",
      "Epoch 1/10\n",
      "900/900 [==============================] - 7s 8ms/sample - loss: 1.9251 - accuracy: 0.3978 - val_loss: 1.2186 - val_accuracy: 0.6200\n",
      "Epoch 2/10\n",
      "900/900 [==============================] - 4s 5ms/sample - loss: 1.1039 - accuracy: 0.6478 - val_loss: 1.0042 - val_accuracy: 0.6800\n",
      "Epoch 3/10\n",
      "900/900 [==============================] - 4s 5ms/sample - loss: 0.6566 - accuracy: 0.7844 - val_loss: 0.6959 - val_accuracy: 0.8100\n",
      "Epoch 4/10\n",
      "900/900 [==============================] - 4s 5ms/sample - loss: 0.3488 - accuracy: 0.9100 - val_loss: 0.9124 - val_accuracy: 0.7500\n",
      "Epoch 5/10\n",
      "900/900 [==============================] - 4s 5ms/sample - loss: 0.2066 - accuracy: 0.9444 - val_loss: 0.8089 - val_accuracy: 0.7800\n",
      "Epoch 6/10\n",
      "900/900 [==============================] - 4s 5ms/sample - loss: 0.1489 - accuracy: 0.9611 - val_loss: 0.9827 - val_accuracy: 0.7800\n",
      "Epoch 7/10\n",
      "900/900 [==============================] - 4s 5ms/sample - loss: 0.1971 - accuracy: 0.9389 - val_loss: 1.0575 - val_accuracy: 0.7500\n",
      "Epoch 8/10\n",
      "900/900 [==============================] - 4s 5ms/sample - loss: 0.0923 - accuracy: 0.9689 - val_loss: 0.8676 - val_accuracy: 0.7900\n",
      "Epoch 9/10\n",
      "900/900 [==============================] - 4s 5ms/sample - loss: 0.1133 - accuracy: 0.9644 - val_loss: 0.8055 - val_accuracy: 0.7800\n",
      "Epoch 10/10\n",
      "900/900 [==============================] - 4s 5ms/sample - loss: 0.0951 - accuracy: 0.9689 - val_loss: 0.8878 - val_accuracy: 0.7800\n"
     ]
    }
   ],
   "source": [
    "histq= q_aware_model.fit(x_train[:1000], y_train[:1000],\n",
    "                        batch_size=128,\n",
    "                        epochs=10,\n",
    "                        verbose=1,\n",
    "                        validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 6s 574us/sample - loss: 1.1339 - accuracy: 0.7428\n",
      "10000/10000 [==============================] - 35s 3ms/sample - loss: 1.6151 - accuracy: 0.6405\n",
      "Baseline test accuracy: 74.27999973297119\n",
      "Quant test accuracy: 64.0500009059906\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    x_test, y_test, verbose=1)\n",
    "\n",
    "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
    "    x_test, y_test, verbose=1)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy*100)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "q_aware_model.save(\"vgg_qat_full_model.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15277856"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "open(\"vgg_qat_full_model.tflite\", \"wb\").write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float complete model in Mb: 174.66024017333984\n",
      "QAT model in Mb: 174.7544708251953\n",
      "QAT compressed model in Mb: 14.570098876953125\n"
     ]
    }
   ],
   "source": [
    "print(\"Float complete model in Mb:\", os.stat('vgg_true_model.h5').st_size/ float(2**20))\n",
    "print(\"QAT model in Mb:\", os.stat('vgg_qat_full_model.h5').st_size/ float(2**20))\n",
    "print(\"QAT compressed model in Mb:\", os.stat('vgg_qat_full_model.tflite').st_size/ float(2**20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float tflite model in Mb: 58.18195343017578\n",
      "Quantized model in Mb: 14.570098876953125\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Create float TFLite model.\n",
    "float_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "float_tflite_model = float_converter.convert()\n",
    "\n",
    "# Measure sizes of models.\n",
    "_, float_file = tempfile.mkstemp('.tflite')\n",
    "_, quant_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(quant_file, 'wb') as f:\n",
    "  f.write(tflite_quant_model)\n",
    "\n",
    "with open(float_file, 'wb') as f:\n",
    "  f.write(float_tflite_model)\n",
    "\n",
    "print(\"Float tflite model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.25\n"
     ]
    }
   ],
   "source": [
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"vgg_qat_full_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test model on some input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "acc=0\n",
    "for i in range(len(x_test)):\n",
    "    input_data = x_test[i].reshape(input_shape)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    if(np.argmax(output_data) == y_test[i]):\n",
    "        acc+=1\n",
    "acc = acc/len(x_test)\n",
    "print(acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize some layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate = tfmot.quantization.keras.quantize_annotate_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "quantize_annotate_22 (Quanti (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 15,250,250\n",
      "Trainable params: 15,250,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "vgg_model = tf.keras.applications.VGG16(input_shape=(32, 32, 3), include_top=False, weights=None)\n",
    "vgg_model.trainable = True\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "#https://github.com/tensorflow/model-optimization/issues/40    \n",
    "model = tf.keras.Sequential()\n",
    "#Had to add input layer, otherwise ran into trouble when printing model summary as well as converting to tf-lite\n",
    "model.add(layers.Input(shape=(32, 32, 3)))\n",
    "input_layer = True\n",
    "for layer in vgg_model.layers:\n",
    "    model.add(layer)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(annotate(layers.Dense(10, activation='softmax')))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7fb1cd798f10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7fb1cd798f10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "quant_dense_5 (QuantizeWrapp (None, 10)                10255     \n",
      "=================================================================\n",
      "Total params: 15,250,255\n",
      "Trainable params: 15,250,250\n",
      "Non-trainable params: 5\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Use `quantize_apply` to actually make the model quantization aware.\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 900 samples, validate on 100 samples\n",
      "Epoch 1/10\n",
      "900/900 [==============================] - 5s 6ms/sample - loss: 0.0744 - accuracy: 0.9767 - val_loss: 1.0120 - val_accuracy: 0.7800\n",
      "Epoch 2/10\n",
      "900/900 [==============================] - 4s 5ms/sample - loss: 0.1263 - accuracy: 0.9678 - val_loss: 1.2246 - val_accuracy: 0.7200\n",
      "Epoch 3/10\n",
      "900/900 [==============================] - 4s 5ms/sample - loss: 0.0924 - accuracy: 0.9756 - val_loss: 1.1501 - val_accuracy: 0.7500\n",
      "Epoch 4/10\n",
      "900/900 [==============================] - 4s 5ms/sample - loss: 0.0842 - accuracy: 0.9778 - val_loss: 1.5330 - val_accuracy: 0.7400\n",
      "Epoch 5/10\n",
      "900/900 [==============================] - 4s 5ms/sample - loss: 0.0507 - accuracy: 0.9844 - val_loss: 1.3601 - val_accuracy: 0.7500\n",
      "Epoch 6/10\n",
      "900/900 [==============================] - 4s 5ms/sample - loss: 0.0805 - accuracy: 0.9800 - val_loss: 1.3084 - val_accuracy: 0.7000\n",
      "Epoch 7/10\n",
      "900/900 [==============================] - 4s 5ms/sample - loss: 0.0652 - accuracy: 0.9789 - val_loss: 1.5899 - val_accuracy: 0.7100\n",
      "Epoch 8/10\n",
      "900/900 [==============================] - 4s 5ms/sample - loss: 0.1078 - accuracy: 0.9722 - val_loss: 1.3783 - val_accuracy: 0.7300\n",
      "Epoch 9/10\n",
      "900/900 [==============================] - 4s 5ms/sample - loss: 0.0801 - accuracy: 0.9778 - val_loss: 1.4986 - val_accuracy: 0.6600\n",
      "Epoch 10/10\n",
      "900/900 [==============================] - 4s 5ms/sample - loss: 0.0784 - accuracy: 0.9800 - val_loss: 1.2040 - val_accuracy: 0.7600\n"
     ]
    }
   ],
   "source": [
    "histq= q_aware_model.fit(x_train[:1000], y_train[:1000],\n",
    "                        batch_size=128,\n",
    "                        epochs=10,\n",
    "                        verbose=1,\n",
    "                        validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 544us/sample - loss: 0.8257 - accuracy: 0.7394\n",
      "10000/10000 [==============================] - 47s 5ms/sample - loss: 1.5860 - accuracy: 0.6652\n",
      "Baseline test accuracy: 73.94000291824341\n",
      "Quant test accuracy: 66.51999950408936\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    x_test, y_test, verbose=1)\n",
    "\n",
    "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
    "    x_test, y_test, verbose=1)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy*100)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15277840"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "open(\"vgg_qat_model.tflite\", \"wb\").write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model in Mb: 58.168704986572266\n",
      "Quantized model in Mb: 14.570083618164062\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Create float TFLite model.\n",
    "float_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "float_tflite_model = float_converter.convert()\n",
    "\n",
    "# Measure sizes of models.\n",
    "_, float_file = tempfile.mkstemp('.tflite')\n",
    "_, quant_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(quant_file, 'wb') as f:\n",
    "  f.write(tflite_quant_model)\n",
    "\n",
    "with open(float_file, 'wb') as f:\n",
    "  f.write(float_tflite_model)\n",
    "\n",
    "print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float complete model in Mb: 62.32586669921875\n",
      "QAT compressed model in Mb: 14.570083618164062\n"
     ]
    }
   ],
   "source": [
    "print(\"Float complete model in Mb:\", os.stat('vgg_model.h5').st_size/ float(2**20))\n",
    "print(\"QAT compressed model in Mb:\", os.stat('vgg_qat_model.tflite').st_size/ float(2**20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.42\n"
     ]
    }
   ],
   "source": [
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"vgg_qat_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test model on some input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "acc=0\n",
    "for i in range(len(x_test)):\n",
    "    input_data = x_test[i].reshape(input_shape)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    if(np.argmax(output_data) == y_test[i]):\n",
    "        acc+=1\n",
    "acc = acc/len(x_test)\n",
    "print(acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
